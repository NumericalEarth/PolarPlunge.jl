{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing with Julia\n",
    "\n",
    "GPUs (Graphics Processing Units) contain **thousands of cores** designed for massively parallel computation.\n",
    "While originally built for rendering graphics, they've become essential for scientific computing,\n",
    "machine learning, and simulations.\n",
    "\n",
    "**In this notebook we'll cover:**\n",
    "- GPU vs CPU architecture\n",
    "- Working with GPU arrays using CUDA.jl\n",
    "- Benchmarking GPU performance\n",
    "- Writing custom GPU kernels with KernelAbstractions.jl\n",
    "- Portable code that runs on both CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setting Up on Google Colab\n",
    "\n",
    "To run these notebooks with GPU support, you can use Google Colab with a free GPU runtime.\n",
    "\n",
    "### Steps:\n",
    "1. Go to [Google Colab](https://colab.research.google.com/)\n",
    "2. Upload this notebook or open it from GitHub\n",
    "3. Go to **Runtime → Change runtime type → Hardware accelerator → GPU**\n",
    "4. Change the runtime to Julia (Julia should already be available on Colab).\n",
    "\n",
    "Alternatively, use [JuliaHub](https://juliahub.com/) which provides free Julia notebooks with GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU Architecture\n",
    "\n",
    "| | CPU | GPU |\n",
    "|--|-----|-----|\n",
    "| Cores | 4-64 | 1000s |\n",
    "| Clock | ~4 GHz | ~1.5 GHz |\n",
    "| Memory BW | ~50 GB/s | ~1000 GB/s |\n",
    "| Strength | Complex logic | Parallel compute |\n",
    "\n",
    "**Key insight:** GPUs trade single-thread performance for massive parallelism.\n",
    "They excel when the same operation is applied to many data elements simultaneously.\n",
    "\n",
    "To make it a bit simpler:\n",
    "![Prof vs Army](images/professor-vs-army.jpeg)\n",
    "\n",
    "**The CPU (The Professor):**\n",
    "Think of it as a brilliant mathematician. It can solve complex, logic-heavy problems one by one, incredibly fast. It focuses on **Low Latency**.\n",
    "- Best for: running your OS, opening apps, and sequential decision-making.\n",
    "\n",
    "**The GPU (The Muscle):**\n",
    "Think of it as an army of thousands of workers. Individually, they aren't Einstein, but together? They can move a mountain in seconds. It focuses on **High Throughput**.\n",
    "- Best for: rendering millions of pixels, training AI models, and matrix multiplication.\n",
    "\n",
    "**In short: you need the Professor to plan the work, and the Army to do the heavy lifting!**\n",
    "\n",
    "---\n",
    "\n",
    "Ok but let's get serious for a second. What's actually going on under the hood?\n",
    "\n",
    "The figure below (from [NVIDIA's CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-programming-guide/)) shows the key architectural difference: a CPU dedicates most of its transistors to **control logic** and **caches**, giving each core maximum single-thread performance. A GPU instead packs the chip with **thousands of small arithmetic units (ALUs)**, sacrificing per-thread speed for raw throughput.\n",
    "\n",
    "![CPU vs GPU Architecture](images/cpu-vs-gpu.png)\n",
    "\n",
    "The important thing to notice here is that the GPU doesn't work alone — **the CPU is always in charge**. It's the one that tells the GPU what to do: it allocates memory, copies data over, launches the compute tasks (called *kernels*), and collects the results. The GPU is powerful, but it's essentially a workhorse waiting for instructions. Without the CPU orchestrating everything, the GPU just sits there doing nothing.\n",
    "\n",
    "So the real workflow looks something like this:\n",
    "1. The **CPU** sets up the problem (allocates arrays, prepares data)\n",
    "2. The **CPU** sends the data to the **GPU**'s memory\n",
    "3. The **CPU** launches a **kernel** — a function that runs on thousands of GPU threads simultaneously\n",
    "4. The **GPU** crunches the numbers in parallel\n",
    "5. The **CPU** pulls the results back when needed\n",
    "\n",
    "This back-and-forth is why you'll often hear people talk about *host* (CPU) and *device* (GPU). The host is the boss, the device is the muscle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "GPU = CUDA\n",
    "GPUArray = CuArray\n",
    "# Check if CUDA is working\n",
    "GPU.functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of our GPU\n",
    "GPU.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuArrays: GPU Arrays\n",
    "\n",
    "CUDA.jl provides `CuArray`, the GPU equivalent of Julia's `Array`.\n",
    "Most array operations \"just work\" on CuArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CPU array\n",
    "a_cpu = rand(Float32, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to GPU\n",
    "a_gpu = GPUArray(a_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directly on GPU (faster - no copy needed)\n",
    "b_gpu = GPU.rand(Float32, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type\n",
    "typeof(a_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Operations\n",
    "\n",
    "Standard Julia array operations work seamlessly on CuArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition\n",
    "c_gpu = a_gpu .+ b_gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "d_gpu = a_gpu * b_gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise functions (broadcasting)\n",
    "e_gpu = sin.(a_gpu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: GPU Array Manipulation\n",
    "\n",
    "Try the following on your own:\n",
    "\n",
    "1. Create a GPU array of size `500 × 500` filled with ones (hint: `CUDA.ones`)\n",
    "2. Create a second GPU array of size `500 × 500` with random values\n",
    "3. Compute the element-wise product of the two arrays\n",
    "4. Compute the sum of all elements (hint: `sum(...)` works on CuArrays)\n",
    "5. Copy the result back to CPU using `Array(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: CPU vs GPU\n",
    "\n",
    "Let's compare matrix multiplication performance.\n",
    "\n",
    "**Note:** `GPU.@sync` ensures the GPU computation finishes before timing completes.\n",
    "GPU operations are asynchronous by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "\n",
    "N = 2000\n",
    "\n",
    "# CPU arrays\n",
    "C_cpu, A_cpu, B_cpu = zeros(Float32, N, N), rand(Float32, N, N), rand(Float32, N, N)\n",
    "\n",
    "# GPU arrays\n",
    "C_gpu, A_gpu, B_gpu = GPUArray(C_cpu), GPUArray(A_cpu), GPUArray(B_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU benchmark\n",
    "@btime LinearAlgebra.mul!($C_cpu, $A_cpu, $B_cpu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU benchmark (CUDA.@sync waits for GPU to finish)\n",
    "@btime GPU.@sync LinearAlgebra.mul!($C_gpu, $A_gpu, $B_gpu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Benchmarking\n",
    "\n",
    "1. Compare CPU vs GPU performance for **element-wise operations** (e.g., `sin.(A)`) instead of matrix multiplication. Use `@btime` and `CUDA.@sync`.\n",
    "2. Try different matrix sizes: `N = 100`, `N = 1000`, `N = 5000`. At what size does the GPU start to win?\n",
    "3. Why might the GPU be slower for small arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom GPU Kernels\n",
    "\n",
    "For full control over GPU computation, we write **kernels** - functions that run on each GPU thread.\n",
    "\n",
    "**KernelAbstractions.jl** provides a portable way to write kernels that work on:\n",
    "- NVIDIA GPUs (CUDA)\n",
    "- AMD GPUs (ROCm)\n",
    "- Intel GPUs (oneAPI)\n",
    "- Apple GPUs (Metal)\n",
    "- CPUs (for testing)\n",
    "\n",
    "This is the approach used by Oceananigans.jl!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding GPU Execution: Threads and Blocks\n",
    "\n",
    "GPUs organize computation into a hierarchy:\n",
    "\n",
    "```\n",
    "Grid (all blocks)\n",
    "  └── Block (group of threads that can synchronize)\n",
    "        └── Thread (single execution unit)\n",
    "```\n",
    "\n",
    "![Grid of Thread Blocks](images/grid_of_thread_blocks.png)\n",
    "*(Image from [NVIDIA's CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-programming-guide/))*\n",
    "\n",
    "For a 256×256 grid, we might use:\n",
    "- **Block size**: 16×16 = 256 threads per block\n",
    "- **Grid size**: 16×16 = 256 blocks\n",
    "- **Total threads**: 256 × 256 = 65,536 (one per grid point)\n",
    "\n",
    "Each thread computes one grid point independently - this is called **SIMD** (Single Instruction, Multiple Data).\n",
    "\n",
    "In KernelAbstractions.jl, you can specify the block size (workgroup) when creating a kernel:\n",
    "```julia\n",
    "kernel = my_kernel!(backend, (16, 16))  # 16×16 workgroup\n",
    "kernel(args..., ndrange=(Nx, Ny))       # total grid size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple 1D Kernel: Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions\n",
    "\n",
    "# A kernel runs on each thread, indexed by @index(Global)\n",
    "@kernel function add_kernel!(C, A, B)\n",
    "    i = @index(Global)            # Get this thread's global index\n",
    "    @inbounds C[i] = A[i] + B[i]  # Perform the addition\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to launch the kernel\n",
    "function gpu_add!(C, A, B)\n",
    "    backend = get_backend(A)          # Detect CPU or GPU\n",
    "    kernel = add_kernel!(backend)     # Compile kernel for this backend\n",
    "    kernel(C, A, B, ndrange=length(A)) # Launch with N threads\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the kernel\n",
    "N = 1024\n",
    "A = GPU.ones(Float32, N)\n",
    "B = GPU.ones(Float32, N) .* 2\n",
    "C = GPU.zeros(Float32, N)\n",
    "\n",
    "gpu_add!(C, A, B)\n",
    "\n",
    "# Verify result (should be all 3s)\n",
    "all(Array(C) .== 3.0f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Write Your Own 1D Kernel\n",
    "\n",
    "Write a kernel `multiply_kernel!` that computes the element-wise product `C[i] = A[i] * B[i]`.\n",
    "\n",
    "1. Define the `@kernel function multiply_kernel!(C, A, B)` following the pattern of `add_kernel!`\n",
    "2. Write a launcher function `gpu_multiply!(C, A, B)` \n",
    "3. Test it: create `A = CuArray(Float32.(1:10))` and `B = CuArray(Float32.(1:10))`, then verify `C[i] = i²`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Kernels\n",
    "\n",
    "For 2D arrays like images or grids, we use 2D indexing.\n",
    "This is essential for PDEs on structured grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Laplacian kernel: ∇²f = ∂²f/∂x² + ∂²f/∂y²\n",
    "# Uses second-order finite differences\n",
    "@kernel function laplacian_kernel!(∇²f, f, Δx, Δy)\n",
    "    i, j = @index(Global, NTuple)  # Get 2D thread index\n",
    "    Nx, Ny = size(f)\n",
    "    @inbounds ∇²f[i, j] = (f[i+1,j] - 2f[i,j] + f[i-1,j]) / Δx^2 + (f[i,j+1] - 2f[i,j] + f[i,j-1]) / Δy^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_laplacian!(∇²f, f, Δx, Δy)\n",
    "    backend = get_backend(f)\n",
    "    kernel = laplacian_kernel!(backend)\n",
    "    kernel(∇²f, f, Δx, Δy, ndrange=size(f))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: f(x,y) = x² + y² has ∇²f = 4\n",
    "Nx, Ny = 64, 64\n",
    "Δx, Δy = 0.1f0, 0.1f0\n",
    "\n",
    "# Create coordinate arrays\n",
    "x = [(i-1) * Δx for i in 1:Nx]\n",
    "y = [(j-1) * Δy for j in 1:Ny]\n",
    "\n",
    "# f(x,y) = x² + y²\n",
    "f_cpu = [x[i]^2 + y[j]^2 for i in 1:Nx, j in 1:Ny]\n",
    "f_gpu = GPUArray(Float32.(f_cpu))\n",
    "\n",
    "# Compute Laplacian\n",
    "∇²f_gpu = GPU.zeros(Float32, Nx, Ny)\n",
    "compute_laplacian!(∇²f_gpu, f_gpu, Δx, Δy)\n",
    "\n",
    "# Check interior points (should be ≈ 4)\n",
    "∇²f_cpu = Array(∇²f_gpu)\n",
    "interior = ∇²f_cpu[2:end-1, 2:end-1]\n",
    "println(\"Expected: 4.0\")\n",
    "println(\"Got (mean): \", sum(interior) / length(interior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient Kernel\n",
    "\n",
    "Write a 2D kernel that computes the **gradient magnitude** of a scalar field using central differences:\n",
    "\n",
    "$$|\\nabla f| = \\sqrt{\\left(\\frac{f_{i+1,j} - f_{i-1,j}}{2\\Delta x}\\right)^2 + \\left(\\frac{f_{i,j+1} - f_{i,j-1}}{2\\Delta y}\\right)^2}$$\n",
    "\n",
    "1. Define `@kernel function gradient_magnitude_kernel!(∇f_mag, f, Δx, Δy)` using 2D indexing\n",
    "2. Test with `f(x,y) = x + 2y` which should give `|∇f| = √(1² + 2²) = √5 ≈ 2.236`\n",
    "\n",
    "**Hint:** use `sqrt(...)` inside the kernel. Remember to only check interior points (avoid boundary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Dispatch in Kernels\n",
    "\n",
    "Julia's multiple dispatch also works inside GPU kernels! This means we can write a **single kernel** that behaves differently depending on a type parameter - no `if/else` branching needed.\n",
    "\n",
    "This is powerful for extensible scientific code: define new physics or numerical schemes as types, and the kernel dispatches to the right method automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an abstract type and two concrete operations\n",
    "struct Square end\n",
    "struct Double end\n",
    "\n",
    "# Dispatch: different methods for different types\n",
    "@inline apply(x, ::Square) = x * x\n",
    "@inline apply(x, ::Double) = x + x\n",
    "\n",
    "# ONE kernel that works with ANY operation type\n",
    "@kernel function apply_operation!(out, input, op)\n",
    "    i = @index(Global)\n",
    "    @inbounds out[i] = apply(input[i], op)\n",
    "end\n",
    "\n",
    "# Test on GPU with both operations\n",
    "N = 1024\n",
    "input_gpu = GPUArray(Float32.(1:N))\n",
    "out_gpu   = similar(input_gpu)\n",
    "\n",
    "backend = get_backend(input_gpu)\n",
    "\n",
    "# Square all elements\n",
    "apply_operation!(backend)(out_gpu, input_gpu, Square(), ndrange=N)\n",
    "println(\"Square: input[3] = 3, output[3] = \", Array(out_gpu)[3])  # 9\n",
    "\n",
    "# Double all elements — same kernel, different type!\n",
    "apply_operation!(backend)(out_gpu, input_gpu, Double(), ndrange=N)\n",
    "println(\"Double: input[3] = 3, output[3] = \", Array(out_gpu)[3])  # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Extend with Multiple Dispatch\n",
    "\n",
    "Add a new operation `Cube` to the dispatch example above:\n",
    "\n",
    "1. Define `struct Cube end`\n",
    "2. Add a method `@inline apply(x, ::Cube) = x * x * x`\n",
    "3. Test it using the *same* `apply_operation!` kernel -- no changes to the kernel needed!\n",
    "4. Verify that `input[3] = 3` gives `output[3] = 27`\n",
    "\n",
    "**Bonus:** Define `struct ScaleBy{T} val::T end` and `@inline apply(x, op::ScaleBy) = x * op.val`. Test with `ScaleBy(10.0f0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portable Code: CPU and GPU\n",
    "\n",
    "The same kernel works on CPU! This is great for:\n",
    "- Testing without a GPU\n",
    "- Debugging (CPU error messages are clearer)\n",
    "- Running on machines without GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions: CPU\n",
    "\n",
    "# Same test, but on CPU\n",
    "f_cpu_array = Float32.(f_cpu)\n",
    "∇²f_cpu_array = zeros(Float32, Nx, Ny)\n",
    "\n",
    "# Uses the SAME kernel code!\n",
    "compute_laplacian!(∇²f_cpu_array, f_cpu_array, Δx, Δy)\n",
    "\n",
    "# Verify\n",
    "interior_cpu = ∇²f_cpu_array[2:end-1, 2:end-1]\n",
    "println(\"CPU result (mean): \", sum(interior_cpu) / length(interior_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: 2D Turbulence\n",
    "\n",
    "Now that we understand GPU computing, we'll apply these concepts to build\n",
    "a 2D incompressible Navier-Stokes solver from scratch in the turbulence lecture notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ne-kernel 1.12",
   "language": "julia",
   "name": "ne-kernel-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
